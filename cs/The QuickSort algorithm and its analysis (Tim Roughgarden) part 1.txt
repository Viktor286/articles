// Tim Roughgarden
// https://www.coursera.org/lecture/algorithms-divide-conquer/quicksort-overview-Zt0Ti

So now we come to one of my favorite sequence of lectures, where we going to discuss the famous QuickSort algorithm. If you ask professional computer scientists and professional programmers to draw up a list of their top five, top ten favorite algorithms, I'll bet you'd see QuickSort on many of those, those peoples' lists. So, why is that? After all, we've already discussed sorting. We already have a quite good and practical sorting algorithm, mainly the Merge Sort algorithm.

Well, QuickSort, in addition to being very practical, it's competitive with, and often superior to, Merge Sort. So, in addition to being very practical, and used all the time in the real world, and in programming libraries, it's just a extremely elegant algorithm. When you see the code, it's just so succinct. It's so elegant, you just sorta wish you had come up with it yourself. Moreover, the mathematical analysis which explains why QuickSort runs so fast, and that mathematical analysis, we'll cover in detail, is very slick. So it's something I can cover in just about half an hour or so.

So more precisely what we'll prove about the QuickSort algorithm is that a suitable randomized implementation runs in time N log N on average. And I'll tell you exactly what I mean by on average, later on in this sequence of lectures. And, moreover, the constants hidden in the Big-Oh notation are extremely small. And, that'll be evident from the analysis that we do.

Finally, and this is one thing that differentiates QuickSort from the merge sort algorithm, is it operates in place. That is, it needs very little additional storage, beyond what's given in the input array, in order to accomplish the goal of sorting. Essentially, what QuickSort does is just repeated swaps within the space of the input array, until it finally concludes with a sorted version of the given array.

The final thing I want to mention on this first slide is that, unlike most of the videos, this set of the videos will actually have an accompanying set of lecture notes, which I've posted on, in PDF, from the course website. Those are largely, redundant. They're optional, but if you want another treatment of what I'm gonna discuss, a written treatment, I encourage you to look at the lecture notes, on the course website. So, for the rest of this video, I'm gonna give you an overview of the ingredients of QuickSort, and what we have to discuss in more detail, and the rest of the lectures will give details of the implementation, as well as the mathematical analysis.

So let's begin by recalling the sorting problem. This is exactly the same problem we discussed back when we covered Merge Sort. So we're given as input an array of n numbers in arbitrary order. So, for example, perhaps the input looks like this array here. And then what do we gotta do? We just gotta output a version of these same numbers but in increasing order. Like when we discussed Merge Sort, I'm gonna make a simplifying assumption just to keep the lectures as simple as possible. Namely I'm going to assume the input array has no duplicates. That is, all of the entries are distinct. And like with the merge sort, I encourage you to think about how you would alter the implementation of QuickSort so that it deals correctly with ties, with duplicate entries.

To discuss how QuickSort works at a high-level, I need to introduce you to the key subroutine, and this is really the, key great idea in QuickSort, which is to use a subroutine which partitions the array around a pivot element. So what does this mean? Well, the first thing you gotta do is, you gotta pick one element in your array to act as a pivot element. Now eventually we'll worry quite a bit about exactly how we choose this magical pivot element. But for now you can just think of it that we pluck out the very first element in the array to act as the pivot. So, for example, in the input array that I mentioned on the previous slide, we could just use "3" as the pivot element.

After you've chosen a pivot element, you then re-arrange the array, and re-arrange it so that every, all the elements which come to the left of the pivot element are less than the pivot, and all the elements which come after the pivot element are greater than the pivot. So for example, given this input array, one legitimate way to rearrange it, so that this holds, is the following. Perhaps in the first two entries, we have the 2 and the 1. Then comes the pivot element. And then comes the elements 4 through 8 in some perhaps jingled order. So notice that the elements to the left of the pivot, the 2 and the 1, are indeed less than the pivot, which is 3. And the five elements to the right of the pivot, to the right of the 3, are indeed all greater than 3.

Notice in the Partition subroutine, we do not insist that we get the relative order correct amongst those elements less than the pivot, or amongst those elements bigger than the pivot. So, in some sense, we're doing some kind of partial sorting. We're just bucketing the elements of the array into one bucket, those less than the pivot, and then a second bucket, those bigger than the pivot. And we don't care about, getting right the order amongst each, within each of those two buckets. So, partitioning is certainly a more modest goal than sorting, but it does make progress toward sorting. In particular, the pivot element itself winds up in its rightful position. That is, the pivot element winds up where it should be in the final sorted version of the array. You'll notice in the example, we chose as the pivot the third largest element, and it does, indeed, wind up in the third position of the array.

So, more generally, where should the pivot be in the final sorted version? Well, it should be to the right of everything less than it. It should be to the left of everything bigger than it. And that's exactly what partitioning does, by definition. So, why is it such a good idea to have a partitioning subroutine? After all, we don't really care about partitioning. What we want to do is sort. Well, the point is that partitioning can be done quickly. It can be done in linear time. And it's a way of making progress toward having a sorted version of an array. And it's gonna enable a divide-and-conquer approach toward sorting the input array.

So, in a little bit more detail, let me tell you about two cool facts about the Partition subroutine. I'm not gonna give you the code for partitioning here. I'm gonna give it to you on the next video. But, here are the two salient properties of the Partition subroutine, discussed in detail in the next video. So the first cool fact is that it can be implemented in linear, that, is O(N) time, where N is the size of the input array, and moreover, not just linear time but linear time with essentially no extra overhead. So we're gonna get a linear time of mutation, where all you do is repeated swaps. You do not allocate any additional memory. And that's key to the practical performance of the QuickSort algorithm.

Secondly, it cuts down the problem size, so it enables the divide-and-conquer approach. Namely, after we've partitioned an array around some pivot elements, all we have to do is recursively sort the elements that lie on the left of the pivot. And recursively sort the elements that lie on the right of the pivot. And then, we'll be done.

So, that leads us to the high-level description of the QuickSort algorithm. Before I give the high-level description, I should mention that this, algorithm was discovered by, Tony Hoare, roughly, 1961 or so. This was at the very beginning of Hoare's career. He was just about 26, 27 years old. He went on to do a lot of other contributions, and, eventually wound up winning the highest honor in computer science, the ACM Turing Award, in 1980. And when you see this code, I'll bet you feel like you wish you had come up with this yourself. It's hard not to be envious of the inventor of this very elegant QuickSort algorithm.
